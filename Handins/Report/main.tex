\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{graphicx}

\title{Environment Lighting Simulation for Augmented Reality Realtime Applications}
\author{Marco Antonio Salcedo Sanson }
\date{December 2016}

% Definition of \maketitle
\makeatletter         
\def\@maketitle{
\noindent
\includegraphics[scale = 0.5]{Figures/uu-logo.png}\\[8ex]
\begin{center}
{\Huge \bfseries \sffamily \@title }\\[4ex] 
{\Large  \@author}\\[4ex] 
\@date\\[8ex]
\end{center}}
\makeatother


\begin{document}

\maketitle

\section{Introduction}
The rendering of virtual objects on current AR applications has a number of shortcomings in the state of the art. It is quite easy to tell that itâ€™s a superimposed virtual object. There are many factors that come into play and give away the lack of feasibility of an AR object, such as marker jiggle, inaccurate depth cues, lack of occlusion from real life objects and incompatible lighting.
The method proposed in this work addresses the latter problem, the non-matching light conditions of the virtual and real world. The method is based on the idea that a 360 photograph of the environment can give us a lot of information about the lighting conditions and that such 360 photographs are now simple to create on an average mobile device. From this 360 photograph it can be determined if the environment is outdoors by day or any other case of illumination. If the environment is indoors the light sources of the room will be approximated using the photo, if outdoors the sun will be assumed as the main source of light, and its position would be calculated using the clock, calendar and compass of the mobile device running the application. 
Whatever the case, the method's output is a set of lights whose properties are a position relative to the camera, a rotation and an intensity.
It's important to stress the fact that the method is proposed for mobile devices. There have been previous works, as detailed in the next section, that deal with the same problem but on computers. Mobile devices present advantages over computers for this task, but also the downside of generally lower processing power compared to computers. This decision comes from the ease of producing the 360 photographs locally, from the judgement that AR is better suited for mobile devices than computers (for mobility and ease of use reasons) and also to be able to exploit the devices geolocation hardware. Of course this doesn't mean that the method will only be applicable to mobile, there are also ways to adapt it to work on computers too.

\section{Related Work}
Agusanto et al. proposed a similar method \cite{agusanto2003}. The main differences in the method they propose have to do mostly with the technologies used. Being already an old paper, their method was conceived for Augmented Reality on a computer, rather than on a smartphone. My choice of a smartphone comes with the downside of lower processing power, but with the great advantage of the different sensors available on the device. The method in the Agusanto paper created the lightmaps using hand-crafted HDR photographs of the real scene using highly reflective metal spheres in contrast with my proposition of using the 360 degree photograph on the device itself.
Pessoa\cite{pessoa2011} expanded on Agusanto's work and created the Real-Time Photorealistic Rendering of Synthetic Objects into Real Scenes (RPR-SORS) toolkit as an extension of the ARToolkit Augmented reality SDK. Although they use more modern techniques, such as cubemaps instead of HDR photographs, their approach still has the same differences than the Agusanto method.
There have been several methods to insert virtual objects into static photographs automatically with highly realistic results. One such example is the method by Karsh \cite{karsh2014} in their Depth transfer: Depth extraction from video using non-parametric sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence paper. These kind of methods are not suitable for real- time applications.

A general different with all the methods reviewed is that none are thought on the context of mobile devices, they are all for AR on a computer. Also none of these methods have attempted to predict the global illumination conditions using the sun position that can be inferred from the device's sensors.

\section{Method}
This section covers the steps needed to produce the wanted results. The input needed is a marker M to provide the object position in the real world; and a 3D mesh O which will be rendered in Augmented Reality. The output is a set of light sources L\textsubscript{i}, each one having a position, orientation and intensity. In order to complete the luminance (luma) analysis a 360 panoramic image is required, but it will be generated as a previous step, it is not necessary to have one beforehand.

1.- Capture a 360 panoramic image of the environment using the device. Desaturate it and lower the color space.

2.- Scan for zones of high luminance. This is defined as a group of white pixels within a imageWidth/100 by imageHeight/100 area.

2b.- High luminance zone found: scan for a falloff around the white pixel group. If found, light source found, will be processed to determine position and orientation. If no falloff, just a clear-colored object.

2c.- If there is a majorly continuous white pixel group encompassing most of the image's upper section, environment is outdoors, only sunlight emulation will be used.

2c.- No high luminance zone found: light source is not visible in the picture, will be approximated analyzing further. This is based around the intuition that it's not possible that there is not a light source, unless the image was pitch black.

3.- Light position and orientation calculation: the input 360 image is mapped on a unit sphere, the preliminary known light sources' positions will be obtained in spherical coordinates in this step. They are then converted to Cartesian coordinates and the distance from the camera will be adjusted once the orientation is known, according to the whiteness of the objects affected by the light. If the light source is not present in frame it will be assumed to be on North Pole of the sphere, because the only positions that a 360 degree photograph would miss are directly above or directly below the camera, and a light source located directly below the camera would be highly unlikely in normal situations.

Orientation: For point lights orientation is irrelevant. For area lights and spotlights the processed input image will be analyzed for white edges.


\section{Implementation}
Here goes the implementation and technical details.

\section{Conclusion}
results seem promising but more research is necessary

\bibliographystyle{plain}
\bibliography{references}
\end{document}
