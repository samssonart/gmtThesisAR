%!TEX root = ../main.tex

In this chapter we will introduce a few technical concepts in order to make the proposed method and what it aims to accomplish more clear.

\section{Augmented Reality}
Augmented Reality is defined as a software system that features a blend of real and virtual objects, the user can interact with it in real time and presents a 2D representation of a 3D world for both the real and virtual sides. This is the most accepted definition originally presented by \cite{azuma1997}. \newline
In order to combine real and virtual objects both worlds must be aligned, in the sense that they must share a common origin for both position and rotation, in a way that they can coexist in a realistic way within the same image. If the virtual objects present in the real scene are not aligned the feasibility of the whole composition is compromised. The tracking of real world features as a guideline for alignment in the virtual world is one of the challenges in the field of AR. There are many techniques that have been used, such as sensor-based tracking, that uses dedicated hardware to feed the software application the position and rotation information it needs; vision-based tracking, in which the position and orientation origin of the virtual world are determined by images recognized by a computer vision algorithm. These images are named \emph{fiducial markers} and can be artificial black and white image that form patterns; more "natural" images featuring points, lines, shapes and/or textures; or even 3D objects can also serve as fiducial markers. Other approaches involving both sensors and vision algorithms have also been used. 
While our method does not aim at the technical challenge of tracking other than existing means, it is important to explain the concept in order to make the narrative clear. The presented information about tracking was gathered from \cite{zhou2008}

\section{Sensors}
State-of-the-art smartphones and tablets in the market have built-in sensors to perform different kinds of measurements that are helpful for application developers, such as motion, orientation and other environmental conditions. The ones that are relevant for this method are the following:\newline
\textbf{Accelerometer}: Measures proper acceleration relative to gravity. Its application in mobile development is to measure motion changes and to the device orientation relative to the Earth's surface. It usually consists of 3 orthogonal axes, and therefore can measure acceleration on one, two or three axes. The output is given in m/s\textsuperscript{2}. \newline
\textbf{Gyroscope}: A sensor that is capable of measuring the rate of rotation around a particular axis. It serves the same purpose as the accelerometer, but mobile devices usually have both for more robust measurements. The output is given in rad/s. In our method these will be used to acquire the $360^{\circ}$  panoramic photograph, making sure that the device is within the same pitch while capturing all the images that will be stitched together to make the panorama. \newline
\textbf{Magnetometer}: It serves the purpose of a compass by measurering the device's orientation with respect to the Earth's magnetic poles. In mobile development it is useful to get the heading of the device expressed in degrees from the north in a clockwise direction.\newline
\textbf{GPS}: Measures the raw position of the device in three-dimensional Cartesian coordinates where the origin is the center of the Earth. It is used to determined where on planet Earth the device is located (Country, city, neighborhood, etc.). It needs to have an uninterrupted line of sight, with no electromagnetic interference, with at least 4 out of the 24 satellites in orbit that are used for GPS.

\section{Standard Illuminant}
In order to describe the color of a light source, it is important to have detailed knowledge of the type of illuminant used. The International Commission on Illumination (CIE)  have defined a number of spectral power distributions, referred to as CIE standard illuminants, to provide reference spectra for colorimetric issues. The illuminants are denoted by a letter or a letter-number combination. Their spectral power distributions (SPD) are normalized to a value of 100 at a wavelength of 560 nm in following figures. Illuminants series A through D exist, all of them specializing on a different kind of light source. For our method the relevant one is series D, because it describes a type of average light source, typically associated to daylight global illumination.\newline
D65 corresponds roughly to the average midday light in western and norther Europe (comprising both direct sunlight and the light diffused by a clear sky). Because of that, it is also called a \emph{daylight illuminant}. It has a correlated colour temperature of approximately $6500 K$. As any standard illuminant is represented as a table of averaged spectrophotometric data, any light source which statistically has the same relative spectral power distribution (SPD) can be considered a D65 light source. It is important to note that D65 is purely theoretical, there are no actual artificial D65 light sources. The D65 is the white point that we used to convert chromatic information into luminance for the light source detection.

\section{Panoramic photography}
Panoramic photography is a technique of photography, using specialized equipment or software, that captures images with horizontally, and sometimes also vertically, elongated fields of view. While there is no formal definition of the minimum field of view required for a photograph to be considered panoramic, in this work we focus on complete horizontal $360^{\circ}$ captures.\newline
There are different kinds of panoramic images, the most intuitive is the one called wide-format photograph. It basically consists of a series of photos taken from the same height and later stitched together to appear as a single wide image. The image can be then projected on to a cylinder. This kind of panoramic photograph captures $360^{\circ}$ of horizontal field of view, while the vertical field of view is dependant entirely on the lens used to capture the individual photographs. Figure 3.1 shows an example of a cylindrical panoramic image. 

\begin{figure}[H]
  \centering
  \setlength{\unitlength}{\textwidth} 
    \begin{picture}(1,0.5)
       \put(-0.1,0){\includegraphics[width=1.3\unitlength]{Figures/pano6.jpg}}
       
    \end{picture}
    \caption{A cylindrical panorama of Trafalgar Square}
\end{figure}

The type of panoramic image used in our method is called equirectangular panoramic image, or spherical panoramic image, due to the fact that the resulting image can be projected on to a sphere. The projection maps meridians to vertical straight lines of constant spacing, and circles of latitude to horizontal straight lines of constant spacing. This kind of projection introduces the types of distortion often seen in spherical projections, such as Mercator, where the poles of the sphere appear bigger than they really are. The process used by Google to create equirectangular panoramic images on mobile devices involves taking several pictures of the environment and, using the device's sensors, project them onto a sphere in their position relative to Earth's north. \newline
Once all pictures are captured they are stitched together in a similar manner as for cylindrical mapping. For each individual image features are identified, those same features are searched on neighboring images and aligned so that they overlap. The area of overlap is then blended, resulting in a seamless, yet distorted representation of all the images as one. After this, the projection is applied as follows: \newline

\begin{equation}
    \lambda = \frac{x}{cos(\phi_l )} + \lambda_0
\end{equation}

\begin{equation}
    \phi = y + \phi_l
\end{equation}

Where: \newline
$\lambda$ is the longitude of the location to project. \newline
$\phi$ is the latitude of the location to project. \newline
$\phi_l$ are the standard parallels (north and south of the equator) where the scale of the projection is true. \newline
$\lambda_0$ is the central meridian of the map. \newline
$x$ is the horizontal coordinate of the projected location on the map. \newline
$y$ is the vertical coordinate of the projected location on the map. \newline

The kind of expected result can be seen on figure 3.2
\begin{figure}[H]
  \centering
  \setlength{\unitlength}{\textwidth} 
    \begin{picture}(0.8,0.5)
       \put(-0.1,0){\includegraphics[width=1.0\unitlength]{Figures/equirectangular.jpg}}
       
    \end{picture}
    \caption{An average indoors equirectangular panorama}
\end{figure}