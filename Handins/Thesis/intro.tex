%!TEX root = ../main.tex

The rendering of virtual objects on current AR applications has a number of shortcomings in the state of the art. It is quite easy to differentiate a superimposed virtual object from a real one, even if they represent the same object. There are many factors that come into play and give away the lack of feasibility of an AR object, such as marker jiggle, inaccurate depth cues, lack of occlusion from real life objects and incompatible lighting.
The method proposed in this work addresses the latter problem, the incompatible light conditions of the virtual and real world. The method is based on the idea that a 360 photograph of the environment can give us a lot of information about the lighting conditions and that such 360 photographs are now simple to create locally on an average mobile device. From this 360 photograph light sources are detected and filtered by intensity, dim lights or sources that are too far away and thus don't contribute to much to the local illumination are not taken into consideration. From a spherical 360 panoramic photograph the direction of the light sources with respect to the device can be calculated. The average color of the light source can also be inferred from the photograph, common light sources are within the light blue or yellow spectrum, but in special cases one could use colored lights with filters or special light bulbs, and having the AR objects reflect those colors would also contribute a lot to the illusion of reality of the overall scene. \newline
The method's output is a set of lights whose properties are a rotation, color, and an intensity.
It's important to stress the fact that the method is proposed for mobile devices. There have been previous works, detailed in the next chapter, that deal with the same problem but on computers. Mobile devices present advantages over computers for this task, but also the downside of generally lower processing power compared to computers. This decision comes from the ease of producing the 360 photographs locally, from the judgement that AR is better suited for mobile devices than computers (for mobility and ease of use reasons) and also to be able to exploit the devices geolocation hardware. Of course this doesn't mean that the method will only be applicable to mobile, there are also ways to adapt it to work on computers too.\newline
The research question from which this method was originally envisioned is: "Can the accuracy of lighting in AR applications be improved using a mobile device to it's full potential?". What this means is that thanks to the many ways a smartphone or tablet can interact with the user and the environment we can know a lot about the surroundings. And this information could potentially be very useful to save computation time, which will make up for the lower computational capabilities and make for a more fair comparison to other methods.\newline
The contributions of this method are:
\begin{itemize}
    \item An image-based method with which the lighting conditions of the entire environment can be approximated
    \item The expansion of an already existing AR tracking library (ARToolkit) to add to the realisim of the rendering.
\end{itemize}