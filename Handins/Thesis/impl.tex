%!TEX root = ../main.tex

In this chapter, the specifics of the implementation made for the demo application are discussed. In general it's a direct translation of the theoretical method proposed there were a few adaptations needed because of the way light is modelled in raster graphics.

\section{Technologies}
The idea of the method is that after the experiments and the eventual would-be finished product can be used within a mobile device with no help from computers. To that end, the implementation uses different technologies. For the capturing of spherical panoramic images the Google Street View app is used, it's available for both iOS and Android and it's a ready-made solution to use panoramic images as a tool for the experiments. The base of the implementation is done in Unity, but a native code library is used within Unity. Said native code library is written in C++ and uses OpenCV for all the image processing tasks. OpenCV is also available as a library for Unity, but it's a paid library, it doesn't have all the functionality that its C++ counterpart has and offloading the heavy image processing parts of the method to native code results in faster execution.\newline
All of these technologies offer platform agnosticism by themselves, however adjustments do have to be made in order to target either iOS or Android. For instance, to be able to use the panoramic images created with Street View another native code plugin was necessary to read images from the device's file system, this plugin would be a completely different implementation from platform to platform. The ARToolKit library is used for the Augmented Reality tasks of tracking markers and providing the virtual world's point of reference. The device used for this particular implementation is an iPad Air tablet running iOS 11, however adjusting the application to support Android devices as well would be a simple task.\newline

\section{Implementation}
The system consists of three components, the Google Street View app to capture the panoramic image, the C++ and OpenCV library to that takes care of most of the pre-calculation steps defined in the method and the Unity application that puts everything together and takes care of the real-time phase of the method. \newline
The OpenCV API has all the necessary functions implemented to apply the mathematical functions proposed in the method section so this part of the implementation was a direct translation into code. In order to achieve the desired graphic quality the implementation uses several tools at Unity's disposal. Reflections are implemented using Reflection Probes for example. Shadows are also baked using Unity, but a custom shader was necesary for the "shadow catcher" plane.\newline
In an attempt to homologate the look of both the real and virtual camera, a full screen effect was applied to the composted scene, in an attempt to make the clean virtual camera match the noisy device's camera. This was done through a Film Grain camera effect.