%!TEX root = ../main.tex
\section{Light detection methods}
\cite{Laskowski2007} came up with a method to situate a pixel in an image that is determined to be the placement of a light source. It's a quite robust method that supports cases where there are no light sources visible and HDR images. The shortcomings of this method are that its response to outdoors photographs where there is only ambient light is not very accurate; and that his algorithm has a big trade-off with accuracy and converge time, there is an optional step involving a Bresenham method which makes results a lot more accurate but also presents a significant slow down. In our method will go beyond this, not just detecting the light sources in the static image, but also calculate their properties in 3D space. \newline
\section{Offline synthetic image composition} 
There have been several methods to insert virtual objects into static photographs automatically with highly realistic results. One such example is the method by \cite{karsh2014}. These kind of methods are not suitable for realtime applications, and they use a single view image to infer the lighting conditions. Our method is more robust in the sense that it uses a $360^{\circ}$ view of the environment to eliminate assumptions of what there is out of frame.\newline
\cite{xing2013} also devised a method to compose virtual objects into static photographs, focusing on outdoors scenes. The method is quite sophisticated, but a lot of the information fed to the light calculation functions involve manual input by the user. In our method the amount of user input besides the panoramic $360^{\circ}$ image is minimized, asking the user only to locate the section of the image the camera will see during execution.\newline
\section{Realtime synthetic image composition}
\cite{agusanto2003} proposed a method similar to what is intended as a whole in the one presented here. The main differences in their method have to do mostly with the technologies used, since it was conceived for Augmented Reality on a computer, rather than on a smartphone. The choice of a smartphone comes with the downside of lower processing power, but with the great advantage of the different sensors available on the device. The method in the Agusanto paper infers the lighting conditions using hand-crafted HDR photographs of the real scene using highly reflective metal spheres in contrast with my proposition of using the $360^{\circ}$ photograph on the device itself, which is a lot more accessible to an average consumer.\newline
\cite{pessoa2011} expanded on Agusanto's work and created the Real-Time Photorealistic Rendering of Synthetic Objects into Real Scenes (RPR-SORS) toolkit as an extension of the ARToolkit Augmented reality SDK. Although they use more modern techniques, such as cubemaps instead of HDR photographs, their approach still has the same differences than the Agusanto method.\newline
Our method is also based on the work by \cite{kanbara2004} to a certain extent. Their method calculates direct lighting on an AR object using a 2D/3D marker. One part is a regular fiducial marker and the other is a small chrome sphere. The light reflections on the sphere are detected and the light direction is approximated using this information. The need of a physical chrome sphere is the main shortcoming of this method. Our method builds on this idea but using a virtual analogy of the chrome sphere consisting of a $360^{\circ}$ panoramic image mapped on a sphere, this has all the advantages of Kanbara's method without the disadvantage of needing an actual reflective sphere.