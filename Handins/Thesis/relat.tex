%!TEX root = ../main.tex

In terms of image-based methods to detect light sources, Laskowski\cite{Laskowski2007} came up with a method to situate a pixel in an image that is determined to be the placement of a light source. It's a quite robust method that supports cases where there are no light sources visible and HDR images. The shortcomings of this method are that its response to outdoors photographs where there is only ambient light is not very accurate; and that his algorithm has a big trade-off with accuracy and converge time, there is an optional step involving a Bresenham method which makes results a lot more accurate but also presents a significant slow down. In this work the method will also go beyond, not just detecting the light sources in the static image, but also calculate their properties in 3D space. 
Agusanto et al.\cite{agusanto2003} proposed a method similar to what is intended as a whole in the one presented here. The main differences in the method they propose have to do mostly with the technologies used. Being already an old paper, their method was conceived for Augmented Reality on a computer, rather than on a smartphone. The choice of a smartphone comes with the downside of lower processing power, but with the great advantage of the different sensors available on the device. The method in the Agusanto paper infers the lighting conditions using hand-crafted HDR photographs of the real scene using highly reflective metal spheres in contrast with my proposition of using the 360 degree photograph on the device itself, which is a lot more accessible to an average consumer.\newline
Pessoa\cite{pessoa2011} expanded on Agusanto's work and created the Real-Time Photorealistic Rendering of Synthetic Objects into Real Scenes (RPR-SORS) toolkit as an extension of the ARToolkit Augmented reality SDK. Although they use more modern techniques, such as cubemaps instead of HDR photographs, their approach still has the same differences than the Agusanto method.\newline
There have been several methods to insert virtual objects into static photographs automatically with highly realistic results. One such example is the method by Karsh \cite{karsh2014} These kind of methods are not suitable for real- time applications, and they use a single view image to infer the lighting conditions. The method proposed here is more robust in the sense that it uses a 360 view of the environment to eliminate assumptions of what there is out of frame.\newline
Xing\cite{xing2013} also devised a method to compose virtual objects into static photographs, focusing on outdoors scenes. The method is quite sophisticated, but a lot of the information fed to the light calculation functions involve manual input by the user. The proposed method is intended to work both for indoors and outdoors scenes, the main difference with the way outdoors scenes will be managed is that the method works automatically, without user intervention. This is accomplished by the usage of the mobile device's dedicated geolocation hardware. By using it, there is no need to assume anything or ask for any user input, the device will be able to tell a good approximation of the sun position using only the sensor's information.\newline
This method is also based on Kanbara's work \cite{kanbara2004} to a certain extent. This 2004 paper proposes a method to calculate direct lighting on an AR object using a 2D/3D marker. One part is a regular fiducial marker and the other is a small chrome sphere. The light reflections on the sphere are detected and from them the light direction is approximated. The need of a physical chrome sphere is the main shortcoming of this method. The method proposed in this work builds on this idea but using a virtual analogy of the chrome sphere consisting of a spherical panoramic image mapped on a sphere, this has all the advantages of Kanbara's method without the disadvantage of needing an actual reflective sphere.