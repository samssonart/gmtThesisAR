%!TEX root = ../main.tex
\section{Light detection methods}
\citep{Laskowski2007} came up with a method to infer which pixels in an image is a direct light source. It is a robust method that supports HDR images and cases where there are no visible light sources. Their method has a number of shortcomings. The response to outdoors photographs, where there is only ambient light, is not very accurate. Another limitation is that their algorithm has a trade-off with accuracy and convergence time, as they explain that there is an optional step involving a Bresenham method which makes results more accurate but also presents a significant slow down. Our method not only calculates the position of light sources within the image, but also approximates their properties in 3D space. \newline
\section{Offline synthetic image composition} 
There have been several methods to insert virtual objects into static photographs automatically with highly realistic results. One such example is \citep{karsh2014}. These methods are not suitable for realtime applications, and they use a single view image to infer the lighting conditions. Our method is more robust in the sense that it uses a $360^{\circ}$ view of the environment and thus uses information of the full environment.\newline
\section{Realtime synthetic image composition}
\citep{agusanto2003} proposed a method related to our methodology. Their method computes the scene lighting using lightmap images. In order to generate such lightmaps they need a digital single-lens reflex camera, software to generate HDR images from multiple exposures, and a light probe in the form of a reflective sphere. Our method proposes using the $360^{\circ}$ photograph on the device itself, which is accessible to the average user.\newline
\citep{pessoa2011} created the Real-Time Photorealistic Rendering of Synthetic Objects into Real Scenes (RPR-SORS) toolkit as an extension of the ARToolkit Augmented Reality SDK. The method behind their toolkit is an updated version of the one in \citet{agusanto2003} that uses cubemaps instead of HDR imagery to improve the rendering. This approach also has the drawback of requiring hand-crafted image maps that involve technical knowledge and dedicated software and hardware to generate.\newline
Our method is also based on the work by \citep{kanbara2004}. Their method calculates direct lighting on an AR object using a 2D/3D marker. One part is a regular fiducial marker and the other is a small chrome sphere. The light reflections on the sphere are detected and the light direction is approximated using this information. The advantage of this method is that it doesn't require pre-generated image maps. But the need for a physical chrome sphere is still a shortcoming. Our method builds on this idea and instead uses a virtual analogy of the chrome sphere consisting of a $360^{\circ}$ panoramic image mapped on a sphere; this has all the advantages of Kanbara's method, without the disadvantage of needing an actual reflective sphere.