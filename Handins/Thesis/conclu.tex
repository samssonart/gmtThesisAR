In this chapter the results are discussed and a conclusion is formulated. The shortcomings of the method and the implementation are also mentioned and taken into consideration for future work.

\section{Discussion}
We can appreciate a very close resemblance among the virtual and real objects, however it's still not a perfect fit. Some of the factors cannot be worked around trivially, for example one of the differences is that there are small imperfections present in the real object that are not there in the virtual one. In order to account for them, a 3D scan of the specific real object would be necessary. 3D scans are often extremely dense meshes that would pose a problem to maintain interactive framerates.\newline
Despite the film grain filter, it's still evident that the virtual camera has better resolution than the real one. The fact that in the virtual world everything looks "too perfect" also breaks the illusion of realism. However, in a real application of the method the user would ideally use AR to view something that is not there in the real world, instead of a side-by-side comparison to a real object. So by not having a clear ground truth of how such an object would look through the camera feed, this problem is mitigated.\newline
Our method is placing lights using a reasonably accurate orientation, but at an admittedly inaccurate distance. After seeing the results we can also conclude that the orientation is far more important than the position, we do see differences in the length of the projected shadow, which is a determined by the distance to the light source, nevertheless this discrepancy is less noticeable than when the angle of the shadow is different with respect to the real object reference.\newline
When virtual shadows overlap a real object that is in front of it the result is not visually pleasing or realistic. In order to prevent this and make the shadow cast on non-planar neighboring objects, or at least have the objects occlude it, some knowledge of the environment in 3D would be necessary, and that would require hardware that is not present on average consumer smartphones.\newline
There are a few known cases where false positives are found by the light detection algorithm. When a light source is directed towards a glossy surface, such as a varnished desk for example, the incidence of the light on such a surface is detected as a light source as well. A similar thing happens sometimes for intense white objects that are not light sources. Although in the first scenario it can be argued that the light bouncing off a glossy surface could indeed be considered a light source. \newline
The system is closed once the runtime phase starts, therefore it lacks adaptation to light changes, due to the nature of the method, having a pre-calculation phase and a runtime phase, it wouldn't be possible to change the light setup in the same way as it was originally captured.\newline
The system limits the amount of lights it simulates to 8 for performance reasons, but even at 8 light sources the performance of the application is low. Between 15 and 24 FPS, it can still be considered interactive, but it's not ideal, specially when taking into account that the application is only doing rendering, a real application of the method, such as a game, would require other layers of complexity that would need processing time.\newline


\section{Conclusion}
All in all, despite the shortcomings of the method, the objective was achieved. The main goals of the method are to approximate the lighting conditions of the environment via a panoramic image and to use this knowledge of the environment to improve the realism of the rendering. Both objectives are met, in the understanding that improvement is not an absolute measure, meaning that as long as the method makes the composted scene more realistic the objective is fulfilled, regardless of being far from perfection still.\newline
There's room for improvement, both on the technical side, on the user experience side and in the results. This will be further developed in the next section.

\section{Future work}
During the development of this method there were several changes to the Augmented Reality landscape through the introduction of new SDK's. ARKit \citep{ARkit} and Tango/ARCore \citep{Tango} are more robust solutions than the ARToolKit we used in this method. While both ARKit and ARCore have an ambient light estimation feature, and ARCore supposedly even has direct illumination estimation, the overlap with what was proposed here is not complete. In a future iteration of the system it would be interesting to see the method working with these new SDK's. Google Tango would be even more interesting, due to the fact that it has a depth sensing camera, so it would even be possible to roughly reconstruct the scene for accurate shadow casting and determine the distance to light sources precisely.\newline
Other than that, a vast improvement would be support for changing light conditions. Most real-world applications for such AR graphics would require interaction outdoors for more than a few minutes, so the lighting conditions are bound to change in the middle of a session.\newline
Just for the sake of maintaining a more uniform user experience it would be a nice addition to implement something similar to the Google Street View spherical panorama capture module within the app. This is something completely off-topic for the method and something not trivial to implement, but if it was there it would be a nice addition nonetheless. 